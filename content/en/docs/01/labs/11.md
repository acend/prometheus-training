---
title: "1.1 Tasks: Setup"
weight: 1
onlyWhenNot: baloise

sectionnumber: 1.1
---


## Task {{% param sectionnumber %}}.1: Getting Started - Web IDE

The first thing we're going to do is to explore our lab environment and get in touch with the different components.

The namespace with the name corresponding to your username is going to be used for all the hands-on labs. And you will be using the folling tools during the lab

* Gitea Git Server: [https://{{% param giteaUrl %}}](https://{{% param giteaUrl %}}/) - Login with  `<user>` and the provided password
* Argo CD Server: [https://{{% param argoCdUrl %}}](https://{{% param argoCdUrl %}}) - Login via Gitea
* git - Login with  `<user>` and the provided password
* {{% param cliToolName %}} - already logged in

{{% alert title="Note" color="primary" %}}The URL and Credentials to the Web IDE will provided by the teacher. Use Chrome for the best experience.{{% /alert %}}


Once you're successfully logged into the web IDE open a new Terminal by hitting `CTRL + SHIFT + ¨` or clicking the Menu button --> Terminal --> new Terminal and check the installed {{% param cliToolName %}}version by executing the following command:

```bash
{{% param cliToolName %}} version --output=yaml
```

The Web IDE Pod consists of the following tools:

* oc
* kubectl
* kustomize
* helm
* kubectx
* kubens
* tekton cli
* argocd

The files in the home directory under `/home/project` are stored in a persistence volume, so please make sure to store all your persistence data in this directory.


### Task {{% param sectionnumber %}}.1.1: Local Workspace Directory

During the lab, you’ll be using local files (eg. YAML resources) which will be applied in your lab project.

Create a new folder for your `<workspace>` in your Web IDE  (for example `prometheus-training` under `/home/project/prometheus-training`). Either you can create it with `right-mouse-click -> New Folder` or in the Web IDE terminal

```bash
mkdir prometheus-training && cd prometheus-training
```

In the Web IDE we set the `USER` environment variable to your personal `<username>`.

Verify that with the following command:
```bash
echo $USER
```

The `USER` variable will be used as part of the commands to make the lab experience more comfortable for you.

Clone the forked repository to your local workspace:

```bash
git clone https://$USER@{{% param giteaUrl %}}/$USER/prometheus-training-lab-setup.git
```

Change the working directory to the cloned git repository:


```bash
cd prometheus-training-lab-setup
```

For convenience let's configure the git client:

```bash
git config user.name "$USER"
git config user.email "$USER@{{% param giteaUrl %}}"
```

And we also want git to store our Password for two days so that we don't need to login every single time we push something.

```bash
git config credential.helper 'cache --timeout=172800'
```

Then use the following command to verify whether the git config for username and email were correctly added:

```bash
git config --local --list
```

Explore the cloned repository.


## Task {{% param sectionnumber %}}.2: Install Prometheus

As explained in the previous section, we're going to use ArgoCD to deploy our Kubernetes resources for our lab, therefore the first thing we do is to create the ArgoCD configuration.

## Configure ArgoCD correctly

TODO Description, why we need this, but that they should not bother too much about it

```bash
{{% param cliToolName %}} -n argocd create -f umbrella-app.yaml
```

For ArgoCD to synchronize applications we need to create a Kubernetes Custom Resource called Application. The Application creates a logical connection between a Kubernetes namespace and a git repository to synchronize.


### Create ArgoCD Application

As mentioned our Prometheus Stack will be deployed in the `<user>-monitoring` namespace.

TODO to deploy Prometheus we simply need to enable it in our helm chart. Describe why we use helm and so on.

Change the file: usermonitoring/values.yaml and set prometheus to true.

The `<user>` must be replaced with your user.

```yaml
user: <user>
# prometheus
prometheus:
  enabled: true
# alertmanager
alertmanager:
  enabled: false
# grafana
grafana:
  enabled: false

# pushgateway
pushgateway:
  enabled: false
# thanos-ruler
ruler:
  enabled: false
# thanos-query
query:
  enabled: false 
```

Optional use helm template to see what this does in K8S resources

```bash
git add .
git commit -m "Enable Prometheus"
git push
```

ArgoCD will now take care of the deployment.

Head over to the [ArgoCD UI](https://{{% param argoCdUrl %}}) and verify that the synchronization process of your application is synced and healthy. As soon as your application is healthy and synced (green status on top) you are good to go and have your prometheus instance ready.

When all has finished syncing, you can inspect your prometheus installation in your namespace:

```bash
{{% param cliToolName %}} -n $USER-monitoring get prometheus prometheus -oyaml
```

### Setup and configure Prometheus

As mentioned in the introduction, setting up a Prometheus on Kubernetes can be done using the Prometheus Operator. We basically need to specify a [Prometheus custome resouce](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheus) and the Operator will do its work.
In the prometheus custom resource's spec block you can find various configuration options:

```yaml
spec:
  enableAdminAPI: true
  evaluationInterval: 30s
  externalLabels:
    monitoring: <user>
  podMonitorNamespaceSelector:
    matchLabels:
      user: <user>
  podMonitorSelector: {}
  portName: web
  probeNamespaceSelector:
    matchLabels:
      user: <user>
  probeSelector: {}
  resources:
    requests:
      memory: 400Mi
  scrapeInterval: 10s
  serviceAccountName: prometheus-<user>
  serviceMonitorNamespaceSelector:
    matchLabels:
      user: <user>
  serviceMonitorSelector: {}
  ```
TODO: Describe the values that we set in the config: Things like scrape interval: (Prometheus is a pull-based monitoring system which means it will reach out to the configured targets and collect the metrics from them (instead of a push-based approach where the targets will push their metrics to the monitoring server). The option `scrape_interval` defines the interval at which Prometheus will collect the metrics for each target)

{{% alert title="Note" color="primary" %}}
We will learn more about other configuration options (`evaluation_interval`, TODO: other settings visible in the CR) later in this training.
{{% /alert %}}

TODO: Alternatively just switch the branch of your repo to XY.

### Check Prometheus

Is your prometheus running? Use your browser to navigate to https://{{% param prometheusUrl %}} . You should now see the Prometheus web UI.


## Task {{% param sectionnumber %}}.3: Deploy example application

Deploy the Acend example Python application, which provides application metrics at `/metrics` by creating the following file (`user-demo/deployment.yaml`) in your repo:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: example-web-python
  name: example-web-python
spec:
  replicas: 1
  selector:
    matchLabels:
      app: example-web-python
  template:
    metadata:
      labels:
        app: example-web-python
    spec:
      containers:
      - image: quay.io/acend/example-web-python
        name: example-web-python
```
TODO Push the new file:

```bash
git add .
git commit -m "Deploy Demo App"
git push
```

Use the following command to verify whether pod `example-web-python` is Ready and Running. (use CTRL C to exit the command)

```bash
{{% param cliToolName %}} -n $USER-monitoring get pod -w
```

We also need to create a Service for the new application. Create a file (`user-demo/service.yaml`) with the following content:
```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: example-web-python
    prometheus-monitoring: 'true'
  name: example-web-python
spec:
  ports:
    - name: http
      port: 5000
      protocol: TCP
      targetPort: 5000
  selector:
    app: example-web-python
  type: ClusterIP
```

TODO Push the new file:

```bash
git add .
git commit -m "Deploy Demo Service"
git push
```

This created a so-called [Kubernetes Service](https://kubernetes.io/docs/concepts/services-networking/service/)

```bash
{{% param cliToolName %}} -n $USER-monitoring get services
```

### Task {{% param sectionnumber %}}.3: Create a ServiceMonitor

Check whether the application metrics are actually exposed by opening a shell within the container and curling the metrics endpoint.

```bash
# get the pod name
{{% param cliToolName %}} -n $USER-monitoring get pod
# exec curl within the pod
{{% param cliToolName %}} -n $USER-monitoring exec -it <pod-name> -- curl http://localhost:5000/metrics
```
Should result in something like:

```promql
# HELP python_gc_objects_collected_total Objects collected during gc
# TYPE python_gc_objects_collected_total counter
python_gc_objects_collected_total{generation="0"} 541.0
python_gc_objects_collected_total{generation="1"} 344.0
python_gc_objects_collected_total{generation="2"} 15.0
...
```

Since our newly deployed application now exposes metrics, the next thing we need to do, is to tell our Prometheus server to scrape metrics from the Kubernetes deployment. In a highly dynamic environment like Kubernetes this is done with so called Service Discovery.

**Task description**:

Create a ServiceMonitor for the example application

* Create a ServiceMonitor, which will configure Prometheus to scrape metrics from the example-web-python application every 30 seconds.

For this to work, you need to ensure:

* The example-web-python Service is labeled correctly and matches the labels you've defined in your ServiceMonitor.
* The port name in your ServiceMonitor configuration matches the port name in the Service definition.
  * hint: check with `{{% param cliToolName %}} -n $USER-monitoring get service example-web-python -o yaml`
* Verify the target in the Prometheus user interface.

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Create the following ServiceMonitor:
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/name: example-web-python
  name: example-web-python-monitor
spec:
  endpoints:
    - interval: 30s
      port: http
      scheme: http
      path: /metrics
  selector:
    matchLabels:
      prometheus-monitoring: 'true'
```
TODO Rolebindings und generell Flow und Descriptions anpassen

Verify that the target gets scraped in the [Prometheus user interface](http://{{% param replacePlaceholder.k8sPrometheus %}}/targets). Target name: `serviceMonitor/<TODO>/example-web-python-monitor/0` (it may take up to a minute for Prometheus to load the new configuration and scrape the metrics).

{{% /details %}}
