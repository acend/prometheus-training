---
title: "7.1 Tasks: Enable and configure Alertmanager"
weight: 2
sectionnumber: 7.1
onlyWhenNot: baloise
---

### Task {{% param sectionnumber %}}.1: Install Alertmanager and Thanosruler

Update your monitoring application (`charts/user-monitoring/values.yaml`) and update the `alertmanager.enabled` and `ruler.enabled` flag to `true`:

`charts/user-monitoring/values.yaml`:
```yaml
user: <user> # Replace me
# prometheus
prometheus:
  enabled: true
# thanos-query
query:
  enabled: true
# grafana
grafana:
  enabled: true
  datasources:
  - name: prometheus
    access: proxy
    editable: false
    type: prometheus
    url: http://prometheus-operated:9090
# blackboxexporter
blackboxexporter:
  enabled: true
# pushgateway
pushgateway:
  enabled: true
# alertmanager
alertmanager:
  enabled: true
# thanos-ruler
ruler:
  enabled: true

```

Commit and push the changes.

Verify the installation and sync process in the [ArgoCD UI](https://{{% param argoCdUrl %}}).
Or execute the following command:

```bash
{{% param cliToolName %}} -n $USER-monitoring get pod
```

This will install two Custom Resources (CR):

```yaml
...
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  labels:
    app.kubernetes.io/name: alertmanager
  name: apps-monitoring
  namespace: <user>-monitoring
spec:
  alertmanagerConfigNamespaceSelector:
    matchNames:
    - <user>-monitoring
  alertmanagerConfigSelector:
  image: quay.io/prometheus/alertmanager:v0.25.0
  replicas: 2
  resources:
    requests:
      cpu: 4m
      memory: 40Mi
  storage:
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 100Mi
```

```yaml
...
apiVersion: monitoring.coreos.com/v1
kind: ThanosRuler
metadata:
  name: thanos-ruler
  labels:
    app.kubernetes.io/name: thanos-ruler
  namespace: <user>-monitoring
spec:
  image: quay.io/thanos/thanos:v0.28.1
  ruleSelector:
    matchLabels:
      role: alerting
  queryEndpoints:
    - dnssrv+_http._tcp.thanos-querier.<user>-monitoring.svc.cluster.local
  alertmanagersConfig:
    key: alertmanager-configs.yaml
    name: thanosruler-alertmanager-config
```

### Task {{% param sectionnumber %}}.3: Enable Alertmanager in Thanos Ruler

The Alertmanager instance we installed before must be configured in Thanos Ruler. Edit the Secret `thanosruler-alertmanager-config` and add the config below. Thanos' config reloader will pick up the changes automatically.

```yaml
...
apiVersion: v1
kind: Secret
metadata:
  name: thanosruler-alertmanager-config
stringData:
  alertmanager-configs.yaml: |-
    alertmanagers:
    - static_configs:
      - "dnssrv+_web._tcp.alertmanager-operated.<user>-monitoring.svc.cluster.local"
      api_version: v2```
```
//FIXME: correct alertmanager service

### Task {{% param sectionnumber %}}.4: Add Alertmanager as monitoring target in Prometheus

{{% alert title="Note" color="primary" %}}
This setup is only suitable for our lab environment. In real life, you must consider how to monitor your monitoring infrastructure:
Having an Alertmanager instance as an Alertmanager AND as a target only in the same Prometheus is a bad idea!
{{% /alert %}}

This is repetition: The Alertmanagers (`alertmanager-operated.<user>-monitoring.svc:9093`) also exposes metrics, which can be scraped by Prometheus.

Create a ServiceMonitor telling Prometheus where to scrape the metrics of Alertmanager.

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Create a new ServiceMonitor object and add it to the Git repo:

```yaml
...
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/name: alertmanager
  name: alertmanager
  namespace: <user>-monitoring
spec:
  endpoints:
  - port: web
  namespaceSelector:
    matchNames:
    - <user>-monitoring
  selector:
    matchLabels:
      operated-alertmanager: "true"
```

Check in the [Prometheus web UI](http://{{% param replacePlaceholder.prometheus %}}/targets) if the target can be scraped.

{{% /details %}}

### Task {{% param sectionnumber %}}.5: Query an Alertmanager metric

After you add the Alertmanager metrics endpoint, you will have huge bunch of different values and identifiers.

Use curl to get a list of all available metrics and query any one from the Alertmanager.

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

To find out which metrics are available for one service you might query its metrics endpoint with `curl`, e.g. for Alertmanager:

```bash
curl alertmanager-operated.<user>-monitoring.svc:9093/metrics
```

Then you get all metrics as follows (shortened), and you can pick whatever you're interested in.

```promql
# HELP alertmanager_alerts How many alerts by state.
# TYPE alertmanager_alerts gauge
alertmanager_alerts{state="active"} 0
alertmanager_alerts{state="suppressed"} 0
# HELP alertmanager_alerts_invalid_total The total number of received alerts that were invalid.
# TYPE alertmanager_alerts_invalid_total counter
alertmanager_alerts_invalid_total{version="v1"} 0
alertmanager_alerts_invalid_total{version="v2"} 0
# HELP alertmanager_alerts_received_total The total number of received alerts.
# TYPE alertmanager_alerts_received_total counter
alertmanager_alerts_received_total{status="firing",version="v1"} 0
alertmanager_alerts_received_total{status="firing",version="v2"} 0
alertmanager_alerts_received_total{status="resolved",version="v1"} 0
alertmanager_alerts_received_total{status="resolved",version="v2"} 0
...
```

{{% /details %}}

### Task {{% param sectionnumber %}}.6: Get all Metrics from Alertmanager

After you successfully configured Prometheus to scrape the Alertmanager you can also query them using PromQL

Write a PromQL query, which selects all metrics exposed by the Alertmanager (`job="alertmanager"`).

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

To do that, we can simply execute a query without a metrics name and only the job label filter `job="alertmanager"`.

```promql
{job="alertmanager"}
```

{{% /details %}}

