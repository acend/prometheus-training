---
title: "7.3 Tasks: Alertrules and alerts"
weight: 2
sectionnumber: 7.3
onlyWhenNot: baloise
---

{{% alert title="Note" color="primary" %}}

For doing the alerting lab it's useful to have a "real" application so that alerts can be provoked. You will use the demo app installed in your  monitoring-demo namespace for this purpose.

The demo app exposes metrics at `http://localhost:8080/metrics` and should already be monitored by Prometheus
//FIXME: Metrics service address

{{% /alert %}}

The Prometheus Operator allows you to configure Alerting Rules (PrometheusRules). This enables Kubernetes users to configure and maintain alerting rules for their projects. Furthermore it is possible to treat Alerting Rules like any other Kubernetes resource and lets you manage them in Helm or Kustomize. A PrometheusRule has the following form:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: <resource-name>
spec:
  <rule definition>
```

See [the Alertmanager documentation](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) for `<rule definition>`


### Task {{% param sectionnumber %}}.1: Add an alerting rule for crashlooping pods

**Task description:**

* Define an alerting rule which sends an alert when a pod in your namespace is crashlooping at least once in the last 5 minutes
* New alarms should be in `pending` state for 15 minutes before they transition to firing
* Add a label `severity` with the value `info`
* Add an annotation `summary` with information about which pods and job is down

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

To add an Alerting rule, create a PrometheusRule resource `training_testrules.yaml` in the monitoring folder of your repository. //FIXME: Correct folder

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: testrules
spec:
  groups:
    - name: pod-rules
      rules:
        - alert: kubePodCrashLooping
          expr: rate(kube_pod_container_status_restarts_total{job="kube-state-metrics",namespace="<user>-monitoring-demo"}[5m]) * 60 * 5 > 0
          for: 15m
          annotations:
            message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
          labels:
            severity: info
```
{{% /details %}}

You can build/verify your Query in your [Thanos Querier UI](http://{{% param replacePlaceholder.thanosquerier %}}). As soon, as you apply the PrometheusRule resource, you should be able to see the alert in your [Thanos Ruler](http://{{% param replacePlaceholder.thanos %}}) implementation.


### Task {{% param sectionnumber %}}.2: Add a an alert for absent targets

**Task description:**

* Define an alerting rule which sends an alert when a target is down. Remember the `up` metric?
* New alarms should be in `pending` state for 2 minutes before they transition to firing
* Add a label `user` with the value `<user>`
* Add an annotation `summary` with information about which instance and job is down

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Either create a separate PrometheuRule resource as you did before, or add the group definition below to your existing PrometheusRule resource.

```yaml
groups:
  - name: job-rules
    rules:
      - alert: target-down
        expr: up == 0
        for: 2m
        labels:
          user: <user>
        annotations:
          summary: Instance {{ $labels.instance }} of job {{ $labels.job }} is down
```

The value in field `for` is the wait time until the active alert gets in state `FIRING`. Before that, the alert is `PENDING` and not yet sent to Alertmanager.

The alert is instrumented with the labels from the metric (e.g. `job`and `instance`). Additional labels can be defined in the rule. Labels can be used in Alertmanager for the routing.

With annotations, additional human-readable information can be attached to the alert.

* In the Prometheus web UI there is an **Alerts** [menu item](http://{{% param replacePlaceholder.prometheus %}}/alerts) which shows you information about the alerts.

{{% /details %}}


### Task {{% param sectionnumber %}}.3: Verify the target down alert

In this task you're going to explore what happens, when a target (our demo application) that exposes metrics fails and stops working.

**Task description:**

* Simulate a failure by killing or stopping the sample application.
* Verify that the sample app no longer exposes metrics
* What do you observe in Prometheus UI or in Alertmanager UI?

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

You can stop the application scaling down the deployment:

```bash
kubectl --namespace <user>-monitoring-demo scale deployment example-web-python --replicas=0
```

And verify that the demo application doesn't expose any metrics anymore.

```bash
curl http://localhost:8080/metrics
```

//FIXME: Metrics service address

```bash
curl: (7) Failed connect to localhost:8080; Connection refused
```

* The Prometheus web UI **Alerts** [menu item](http://{{% param replacePlaceholder.prometheus %}}/alerts) shows you information about inactive and active alerts.
* As soon as an alert is in state `FIRING` the alert is sent to Alertmanager. You should see the alert in its [web UI](http://{{% param replacePlaceholder.alertmanager %}}).
* You can also check the mail in [mailcatcher](http://{{% param replacePlaceholder.mailcatcher %}})

{{% /details %}}


### Task {{% param sectionnumber %}}.4: Silencing alerts

Sometimes the huge amount of alerts can be overwhelming, or you're currently working on fixing an issue, which triggers an alert. Or you're simply testing something that fires alerts.

In such cases alert **silencing** can be very helpful.

Let's now silence our test alert.

Open the [Alertmanger web UI](http://{{% param replacePlaceholder.alertmanager %}}) and search for the test alert.

{{% alert title="Note" color="primary" %}}
The alert might have been resolved already, use the following command to re-trigger it again:

```bash
user=<user>
kubectl --namespace $user-monitoring exec -it sts/alertmanager-apps-monitoring -- sh
amtool alert add --alertmanager.url=http://localhost:9093 alertname=Up node=bar
```

{{% /alert %}}

You can either silence the specific alert by simply clicking on the `Silcence` button next to the alert, or create a new silence by clicking the `New Silence` button in the top menu on the right.
Either way, you'll end up on the same form. The button next to the alert will conveniently fill out the matchers, so that the alert will be affected by the new silence.

* Click the `Silence` button next to the test alert.
* Make sure the matchers contains the two labels (`alertname="Up"`, `node="bar"`) of the test alert.
* Set the duration to 1h
* Add your username to the creator form field.
* Fill out the description with the reason you're creating a silence.

You can then use the `Preview Alerts` button to check your matchers and create the alert by clicking `create`.

![Alert Silencing](../alert-new-silence.png)

All alerts, which match the defined labels of the matcher will be silenced for the defined time slot.

Go back to the Alerts page, the silenced alert disappeared and will only be visible when checking the silenced alerts checkbox.

The top menu entry silence will show you a list of the created silences. Silences can also be created programmatically using the API or the amtool (`amtool silence --help`).

The following command is exactly the same you just did via the Web UI:

```bash
user=<user>
kubectl --namespace $user-monitoring exec -it sts/alertmanager-apps-monitoring -- sh
amtool silence add alertname=Up node=bar --author="<username>" --comment="I'm testing the silences" --alertmanager.url=http://localhost:9093
```

